{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!unzip data"
      ],
      "metadata": {
        "id": "bqI3DWMZoh6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrNmGuA_i3zs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import cv2\n",
        "import os\n",
        "from imutils import paths\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers.legacy import SGD\n",
        "from tensorflow.keras import backend as K"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load(paths, verbose=-1):\n",
        "    data = list()\n",
        "    labels = list()\n",
        "    # loop over the input images\n",
        "    for (i, imgpath) in enumerate(paths):\n",
        "        # load the image and extract the class labels\n",
        "        im_gray = cv2.imread(imgpath, cv2.IMREAD_GRAYSCALE)\n",
        "        image = np.array(im_gray).flatten()\n",
        "        label = imgpath.split(os.path.sep)[-2]\n",
        "        # scale the image to [0, 1] and add to list\n",
        "        data.append(image/255)\n",
        "        labels.append(label)\n",
        "        # show an update every `verbose` images\n",
        "        if verbose > 0 and i > 0 and (i + 1) % verbose == 0:\n",
        "            print(\"[INFO] processed {}/{}\".format(i + 1, len(paths)))\n",
        "    # return a tuple of the data and labels\n",
        "    return data, labels"
      ],
      "metadata": {
        "id": "-tliQM34kB-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#declear path to your mnist data folder\n",
        "img_path = 'data/trainingSet'\n",
        "\n",
        "#get the path list using the path object\n",
        "image_paths = list(paths.list_images(img_path))\n",
        "\n",
        "#apply our function\n",
        "image_list, label_list = load(image_paths, verbose=10000)\n",
        "\n",
        "#binarize the labels\n",
        "lb = LabelBinarizer()\n",
        "label_list = lb.fit_transform(label_list)\n",
        "\n",
        "#split data into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(image_list,\n",
        "                                                    label_list,\n",
        "                                                    test_size=0.1,\n",
        "                                                    random_state=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvpBWvHTzc06",
        "outputId": "6b9f864d-3431-4808-e01c-006b2772d605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] processed 10000/42000\n",
            "[INFO] processed 20000/42000\n",
            "[INFO] processed 30000/42000\n",
            "[INFO] processed 40000/42000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_clients(image_list, label_list, num_clients=10, initial='clients'):\n",
        "    ''' return: a dictionary with keys clients' names and value as\n",
        "                data shards - tuple of images and label lists.\n",
        "        args:\n",
        "            image_list: a list of numpy arrays of training images\n",
        "            label_list:a list of binarized labels for each image\n",
        "            num_client: number of fedrated members (clients)\n",
        "            initials: the clients'name prefix, e.g, clients_1\n",
        "\n",
        "    '''\n",
        "\n",
        "    #create a list of client names\n",
        "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
        "\n",
        "    #randomize the data\n",
        "    data = list(zip(image_list, label_list))\n",
        "    random.shuffle(data)\n",
        "\n",
        "    #shard data and place at each client\n",
        "    size = len(data)//num_clients\n",
        "    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
        "\n",
        "    #number of clients must equal number of shards\n",
        "    assert(len(shards) == len(client_names))\n",
        "\n",
        "    return {client_names[i] : shards[i] for i in range(len(client_names))}"
      ],
      "metadata": {
        "id": "3cgDez6_4ydA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create clients\n",
        "clients = create_clients(X_train, y_train, num_clients=10, initial='client')"
      ],
      "metadata": {
        "id": "DIAV7gMaASFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_data(data_shard, bs=32):\n",
        "    '''Takes in a clients data shard and create a tfds object off it\n",
        "    args:\n",
        "        shard: a data, label constituting a client's data shard\n",
        "        bs:batch size\n",
        "    return:\n",
        "        tfds object'''\n",
        "    #seperate shard into data and labels lists\n",
        "    data, label = zip(*data_shard)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
        "    return dataset.shuffle(len(label)).batch(bs)"
      ],
      "metadata": {
        "id": "E2l3TyoSEbkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#process and batch the training data for each client\n",
        "clients_batched = dict()\n",
        "for (client_name, data) in clients.items():\n",
        "    clients_batched[client_name] = batch_data(data)\n",
        "\n",
        "#process and batch the test set\n",
        "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))"
      ],
      "metadata": {
        "id": "jkxIFfHQEpaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleMLP:\n",
        "    @staticmethod\n",
        "    def build(shape, classes):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(200, input_shape=(shape,)))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Dense(200))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Dense(classes))\n",
        "        model.add(Activation(\"softmax\"))\n",
        "        return model"
      ],
      "metadata": {
        "id": "JKcMd-iAGmfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.01\n",
        "comms_round = 100\n",
        "loss='categorical_crossentropy'\n",
        "metrics = ['accuracy']\n",
        "optimizer = SGD(lr=lr,\n",
        "                decay=lr / comms_round,\n",
        "                momentum=0.9\n",
        "               )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y69BiKlJHGsx",
        "outputId": "9e498916-6925-4856-836f-fd224e508b6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def weight_scalling_factor(clients_trn_data, client_name):\n",
        "    client_names = list(clients_trn_data.keys())\n",
        "    #get the bs\n",
        "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
        "    #first calculate the total training data points across clinets\n",
        "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
        "    # get the total number of data points held by a client\n",
        "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
        "    return local_count/global_count\n",
        "\n",
        "\n",
        "def scale_model_weights(weight, scalar):\n",
        "    '''function for scaling a models weights'''\n",
        "    weight_final = []\n",
        "    steps = len(weight)\n",
        "    for i in range(steps):\n",
        "        weight_final.append(scalar * weight[i])\n",
        "    return weight_final\n",
        "\n",
        "\n",
        "\n",
        "def sum_scaled_weights(scaled_weight_list):\n",
        "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
        "    avg_grad = list()\n",
        "    #get the average grad accross all client gradients\n",
        "    for grad_list_tuple in zip(*scaled_weight_list):\n",
        "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
        "        avg_grad.append(layer_mean)\n",
        "\n",
        "    return avg_grad\n",
        "\n",
        "\n",
        "def test_model(X_test, Y_test,  model, comm_round):\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    #logits = model.predict(X_test, batch_size=100)\n",
        "    logits = model.predict(X_test)\n",
        "    loss = cce(Y_test, logits)\n",
        "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
        "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
        "    return acc, loss"
      ],
      "metadata": {
        "id": "E1O32ZSrJQIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize global model\n",
        "smlp_global = SimpleMLP()\n",
        "global_model = smlp_global.build(784, 10)\n",
        "\n",
        "#commence global training loop\n",
        "for comm_round in range(comms_round):\n",
        "\n",
        "    # get the global model's weights - will serve as the initial weights for all local models\n",
        "    global_weights = global_model.get_weights()\n",
        "\n",
        "    #initial list to collect local model weights after scalling\n",
        "    scaled_local_weight_list = list()\n",
        "\n",
        "    #randomize client data - using keys\n",
        "    client_names= list(clients_batched.keys())\n",
        "    random.shuffle(client_names)\n",
        "\n",
        "    #loop through each client and create new local model\n",
        "    for client in client_names:\n",
        "        smlp_local = SimpleMLP()\n",
        "        local_model = smlp_local.build(784, 10)\n",
        "        local_model.compile(loss=loss,\n",
        "                      optimizer=optimizer,\n",
        "                      metrics=metrics)\n",
        "\n",
        "        #set local model weight to the weight of the global model\n",
        "        local_model.set_weights(global_weights)\n",
        "\n",
        "        #fit local model with client's data\n",
        "        local_model.fit(clients_batched[client], epochs=1, verbose=0)\n",
        "\n",
        "        #scale the model weights and add to list\n",
        "        scaling_factor = weight_scalling_factor(clients_batched, client)\n",
        "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
        "        scaled_local_weight_list.append(scaled_weights)\n",
        "\n",
        "        #clear session to free memory after each communication round\n",
        "        K.clear_session()\n",
        "\n",
        "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
        "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
        "\n",
        "    #update global model\n",
        "    global_model.set_weights(average_weights)\n",
        "\n",
        "    #test global model and print out metrics after each communications round\n",
        "    for(X_test, Y_test) in test_batched:\n",
        "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEvvDw4eJUcn",
        "outputId": "bd300cc7-97e8-47d6-9414-1c2a4fc06695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 0 | global_acc: 88.905% | global_loss: 1.667275309562683\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 1 | global_acc: 90.976% | global_loss: 1.6164813041687012\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 2 | global_acc: 91.976% | global_loss: 1.596150517463684\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 3 | global_acc: 92.524% | global_loss: 1.584485411643982\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 4 | global_acc: 92.881% | global_loss: 1.575182557106018\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 5 | global_acc: 93.571% | global_loss: 1.5672492980957031\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 6 | global_acc: 93.762% | global_loss: 1.5624721050262451\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 7 | global_acc: 94.214% | global_loss: 1.5581512451171875\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 8 | global_acc: 94.357% | global_loss: 1.5551795959472656\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 9 | global_acc: 94.548% | global_loss: 1.5515999794006348\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 10 | global_acc: 94.833% | global_loss: 1.5488914251327515\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 11 | global_acc: 95.119% | global_loss: 1.5463294982910156\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 12 | global_acc: 95.214% | global_loss: 1.54403555393219\n",
            "132/132 [==============================] - 1s 10ms/step\n",
            "comm_round: 13 | global_acc: 95.333% | global_loss: 1.5418624877929688\n",
            "132/132 [==============================] - 1s 4ms/step\n",
            "comm_round: 14 | global_acc: 95.357% | global_loss: 1.5398386716842651\n",
            "132/132 [==============================] - 1s 4ms/step\n",
            "comm_round: 15 | global_acc: 95.524% | global_loss: 1.5386840105056763\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 16 | global_acc: 95.619% | global_loss: 1.5367634296417236\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 17 | global_acc: 95.524% | global_loss: 1.535801887512207\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 18 | global_acc: 95.738% | global_loss: 1.5339678525924683\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 19 | global_acc: 95.690% | global_loss: 1.532707691192627\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 20 | global_acc: 95.762% | global_loss: 1.5318819284439087\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 21 | global_acc: 95.833% | global_loss: 1.5310231447219849\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 22 | global_acc: 95.905% | global_loss: 1.530097246170044\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 23 | global_acc: 95.833% | global_loss: 1.528984785079956\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 24 | global_acc: 95.905% | global_loss: 1.5283043384552002\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 25 | global_acc: 95.905% | global_loss: 1.527295470237732\n",
            "132/132 [==============================] - 1s 4ms/step\n",
            "comm_round: 26 | global_acc: 95.976% | global_loss: 1.5268471240997314\n",
            "132/132 [==============================] - 1s 4ms/step\n",
            "comm_round: 27 | global_acc: 96.119% | global_loss: 1.525797724723816\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 28 | global_acc: 96.262% | global_loss: 1.5252336263656616\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 29 | global_acc: 96.071% | global_loss: 1.524536371231079\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 30 | global_acc: 96.119% | global_loss: 1.5244505405426025\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 31 | global_acc: 96.286% | global_loss: 1.5234345197677612\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 32 | global_acc: 96.167% | global_loss: 1.523101568222046\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 33 | global_acc: 96.286% | global_loss: 1.5222461223602295\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 34 | global_acc: 96.381% | global_loss: 1.5218887329101562\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 35 | global_acc: 96.381% | global_loss: 1.5213500261306763\n",
            "132/132 [==============================] - 1s 4ms/step\n",
            "comm_round: 36 | global_acc: 96.381% | global_loss: 1.5210163593292236\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 37 | global_acc: 96.452% | global_loss: 1.5204403400421143\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 38 | global_acc: 96.500% | global_loss: 1.5201787948608398\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 39 | global_acc: 96.429% | global_loss: 1.5198158025741577\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 40 | global_acc: 96.476% | global_loss: 1.5191915035247803\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 41 | global_acc: 96.452% | global_loss: 1.5190203189849854\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 42 | global_acc: 96.500% | global_loss: 1.518418788909912\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 43 | global_acc: 96.524% | global_loss: 1.5182877779006958\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 44 | global_acc: 96.548% | global_loss: 1.517905831336975\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 45 | global_acc: 96.571% | global_loss: 1.5177106857299805\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 46 | global_acc: 96.619% | global_loss: 1.517366886138916\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 47 | global_acc: 96.524% | global_loss: 1.5171091556549072\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 48 | global_acc: 96.571% | global_loss: 1.5165915489196777\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 49 | global_acc: 96.619% | global_loss: 1.5164583921432495\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 50 | global_acc: 96.548% | global_loss: 1.5165311098098755\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 51 | global_acc: 96.619% | global_loss: 1.5160107612609863\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 52 | global_acc: 96.667% | global_loss: 1.515600562095642\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 53 | global_acc: 96.619% | global_loss: 1.5155185461044312\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 54 | global_acc: 96.643% | global_loss: 1.5152887105941772\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 55 | global_acc: 96.667% | global_loss: 1.5150883197784424\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 56 | global_acc: 96.714% | global_loss: 1.5146591663360596\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 57 | global_acc: 96.786% | global_loss: 1.5144654512405396\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 58 | global_acc: 96.762% | global_loss: 1.5142298936843872\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 59 | global_acc: 96.738% | global_loss: 1.5139573812484741\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 60 | global_acc: 96.738% | global_loss: 1.513813853263855\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 61 | global_acc: 96.714% | global_loss: 1.5135654211044312\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 62 | global_acc: 96.786% | global_loss: 1.5133988857269287\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 63 | global_acc: 96.762% | global_loss: 1.5133088827133179\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 64 | global_acc: 96.810% | global_loss: 1.5132259130477905\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 65 | global_acc: 96.714% | global_loss: 1.5128943920135498\n",
            "132/132 [==============================] - 1s 4ms/step\n",
            "comm_round: 66 | global_acc: 96.810% | global_loss: 1.5128955841064453\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 67 | global_acc: 96.833% | global_loss: 1.512672781944275\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 68 | global_acc: 96.786% | global_loss: 1.5125139951705933\n",
            "132/132 [==============================] - 1s 4ms/step\n",
            "comm_round: 69 | global_acc: 96.762% | global_loss: 1.5125857591629028\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 70 | global_acc: 96.762% | global_loss: 1.51226007938385\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 71 | global_acc: 96.786% | global_loss: 1.511976957321167\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 72 | global_acc: 96.881% | global_loss: 1.5118058919906616\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 73 | global_acc: 96.857% | global_loss: 1.5116673707962036\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 74 | global_acc: 96.881% | global_loss: 1.5114315748214722\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 75 | global_acc: 96.857% | global_loss: 1.5113784074783325\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 76 | global_acc: 96.833% | global_loss: 1.5113157033920288\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 77 | global_acc: 96.857% | global_loss: 1.5111083984375\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 78 | global_acc: 96.833% | global_loss: 1.5111570358276367\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 79 | global_acc: 96.833% | global_loss: 1.5110251903533936\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 80 | global_acc: 96.810% | global_loss: 1.510820746421814\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 81 | global_acc: 96.881% | global_loss: 1.5107771158218384\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 82 | global_acc: 96.833% | global_loss: 1.5107090473175049\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 83 | global_acc: 96.976% | global_loss: 1.5103309154510498\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 84 | global_acc: 96.929% | global_loss: 1.5102977752685547\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 85 | global_acc: 96.929% | global_loss: 1.5101011991500854\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 86 | global_acc: 96.833% | global_loss: 1.5101144313812256\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 87 | global_acc: 96.929% | global_loss: 1.5099503993988037\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 88 | global_acc: 96.976% | global_loss: 1.5100337266921997\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 89 | global_acc: 96.929% | global_loss: 1.5096228122711182\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 90 | global_acc: 96.905% | global_loss: 1.5095232725143433\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 91 | global_acc: 96.929% | global_loss: 1.5094988346099854\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 92 | global_acc: 96.952% | global_loss: 1.50938880443573\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 93 | global_acc: 96.952% | global_loss: 1.5092512369155884\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 94 | global_acc: 96.952% | global_loss: 1.5090625286102295\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 95 | global_acc: 96.952% | global_loss: 1.5089823007583618\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 96 | global_acc: 96.976% | global_loss: 1.5089240074157715\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 97 | global_acc: 96.976% | global_loss: 1.50874924659729\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 98 | global_acc: 96.952% | global_loss: 1.5088199377059937\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 99 | global_acc: 96.905% | global_loss: 1.508670687675476\n"
          ]
        }
      ]
    }
  ]
}